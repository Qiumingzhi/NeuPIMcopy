#include "StageProgram.h"

#include <cstdint>
#include <iterator>
#include <memory>
#include <vector>

#include "Common.h"
#include "Model.h"
#include "SimulationConfig.h"
#include "Stat.h"
#include "tensor/BTensor.h"
#include "tensor/NPUTensor.h"
#include "tensor/NPUTensorInner.h"
#include "tensor/PIMTensor.h"

StageProgram::StageProgram(Ptr<Model> model,
                           Ptr<BatchedRequest> batched_request,
                           StagePlatform stage_platform, Stage stage)
    : _model(model), _breq(batched_request), _stage_platform(stage_platform),
      _stage(stage), _name(stagePlatformToString(stage_platform) + "_stage_" +
                           stageToString(stage)) {
  this->init_program();
}

// clang-format off
// |     |     A    |     B    |         C        |         D        |     E     |     F     |
// |-----|:--------:|:--------:|:----------------:|:----------------:|:---------:|:---------:|
// |  SA | QKVgen#1 | QKVgen#2 | Pj/FFNs/QKVgen#1 | Pj/FFNs/QKVgen#2 | Pj/FFNs#1 | Pj/FFNs#2 |
// | PIM |     -    |  MHA#1   | MHA#2            | MHA#1            |   MHA#2   |     -     |
// clang-format on
void StageProgram::init_program() {
  assert(_stage != Stage::Finish);

  if (_breq->_reqs.size() == 0) {
    std::string yellow = "\033[1;33m";
    std::string reset = "\033[0m";
    spdlog::info("{}No request in this batch skip{}", yellow, reset);
    return;
  }

  if (_stage_platform == StagePlatform::PIM) {
    if (skip_pim_stage()) {
      std::string yellow = "\033[1;33m";
      std::string reset = "\033[0m";
      spdlog::info("{}PIM: skip{}", yellow, reset);
      return;
    } else
      init_PIM_program();
  } else if (_stage_platform == StagePlatform::SA)
    init_SA_program();
}

bool StageProgram::skip_pim_stage() {
  return _stage == Stage::A ||
         _stage == Stage::F; // 在 Stage A 和 Stage F，PIM（存内计算）不工作。
}

bool StageProgram::enable_proj_ffns() {
  return _stage == Stage::C || _stage == Stage::D || _stage == Stage::E ||
         _stage == Stage::F; // 在 Stage C, D, E, F，开启 Projection（投影） 和
                             // FFN（前馈网络） 计算
}

bool StageProgram::enable_qkv_gen() {
  return _stage == Stage::A || _stage == Stage::B || _stage == Stage::C ||
         _stage == Stage::D; //在 Stage A, B, C, D，开启 QKV 生成计算
}

void StageProgram::init_SA_program() {
  spdlog::info(">>> Initialize SystolicArray Stage Model Program <<<");
  auto N = _breq->get_num_rows();
  spdlog::info("N: {}", N);

  auto E = Config::global_config.model_n_embd;

  bool lets_proj_ffns = enable_proj_ffns();
  bool lets_qkvgen = enable_qkv_gen();

  std::vector<uint32_t> input_dim{N, E};

  //第一次判断：
  //目的: 调整输入张量的形状。
  //如果当前是 Projection/FFN 阶段，输入数据来自 Attention 的输出。由于
  // Attention 是多头并行的，每个 TP卡只拿到了一部分结果，
  // 所以输入维度是切分后的 E / n_tp。 如果不是（即 QKVGen 阶段），
  // 输入是完整的 Embedding，维度是 E。

  if (lets_proj_ffns) { //如果当前 Stage 包含 Projection/FFN (lets_proj_ffns
                        //为真)，说明这是 Attention 之后的计算。
    input_dim[1] /= Config::global_config.n_tp;
    //在多卡张量并行（TP）模式下，Attention 的输出通常是被切分的（每个 TP
    //卡只计算一部分 Head）。因此，输入张量的宽度（Embedding 维度）需要除以 TP
    //并行度 n_tp，变成 [N, E / n_tp]。
  }
  auto input = std::make_shared<NPUTensor>("input", input_dim,
                                           NPUTensorBufType::ACT, true);
  std::vector<Ptr<BTensor>> inputs{input};
  //创建一个名为 "input" 的 NPUTensor 对象，类型为 ACT (Activation)，标记为
  // produced (已生成/存在)。 将其放入 inputs 向量中，作为后续计算块（如
  // projection_block 或 qkv_gen_block) 的输入参数。

  // 第二次判断目的: 构建计算图。
  // 如果当前是 Projection/FFN 阶段，才真正往计算图中添加 projection_block 和
  // ffn1_block 这些算子。
  if (lets_proj_ffns) {
    // >>> Stage: C/D/E/F : Projection + FFN1
    inputs = projection_block(inputs);
    inputs = ffn_block(inputs); // FFN1
    std::string yellow = "\033[1;33m";
    std::string reset = "\033[0m";
    spdlog::info("{}SA : Projection + FFN {}", yellow, reset);
    // <<< Stage: C/D/E/F
  }

  if (lets_qkvgen) {
    // >>> Stage: A/B/C/D : QKVGen
    inputs = qkv_gen_block(inputs);

    std::string yellow = "\033[1;33m";
    std::string reset = "\033[0m";
    spdlog::info("{}SA : QKV generation{}", yellow, reset);
    // <<< Stage:: A/B/C/D
  }

  find_executable_node(input);
}

void StageProgram::init_PIM_program() {
  spdlog::info(">>> Initialize PIM Stage Model Program <<<");
  std::string yellow = "\033[1;33m";
  std::string reset = "\033[0m";
  spdlog::info("{}PIM: MHA{}", yellow, reset);
  Ptr<NPUTensor> query;
  std::vector<Ptr<BTensor>> inputs;

  int sub_batch_size = _breq->_reqs.size();

  uint32_t num_heads =
      Config::global_config.model_n_head / Config::global_config.n_tp;
  uint32_t dk = Config::global_config.model_n_embd /
                Config::global_config.model_n_head; // 64;

  std::vector<Ptr<BTensor>> querys;
  std::vector<Ptr<BTensor>> keys;
  std::vector<Ptr<BTensor>> values;

  for (int j = 0; j < sub_batch_size; j++) {
    /* - [] todo: change query to real query from gkv gen */
    Ptr<InferRequest> request = _breq->_reqs[j];
    int q_len = request->is_initiated ? 1 : request->input_size;
    assert(q_len == 1);

    query = std::make_shared<NPUTensor>(
        "query",
        std::vector<uint32_t>{num_heads, static_cast<unsigned int>(q_len), dk},
        NPUTensorBufType::ACT, true);
    querys.push_back(query);

    /* key/value cache */
    keys.push_back(request->K_cache[0]);
    values.push_back(request->V_cache[0]);
  }

  /* gemv + softmax */
  std::vector<Ptr<BTensor>> mha_pim_inputs = querys;
  mha_pim_inputs.insert(mha_pim_inputs.end(), keys.begin(),
                        keys.end()); // querys, keys

  auto logit_softmax = add_op(std::make_shared<NeuPIMSLogitSoftmax>(name_gen(
      LAYER(0), BlockType::Attention, OperationType::NeuPIMSLogitSoftmax)));
  inputs = get_outputs(logit_softmax, mha_pim_inputs);

  /* pim_gemv + add */
  inputs.insert(inputs.end(), values.begin(), values.end()); // logits, values

  auto attend = add_op(std::make_shared<NeuPIMSAttend>(
      name_gen(LAYER(0), BlockType::Attention, OperationType::NeuPIMSAttend)));
  inputs = get_outputs(attend, inputs);

  find_executable_node(query);
}

Ptr<Operation> StageProgram::add_op(std::shared_ptr<Operation> op) {
  spdlog::info("operation {} added. add_op", op->get_name());
  _op_map[op->get_id()] = op;
  return op;
}

std::vector<Ptr<BTensor>>
StageProgram::get_outputs(Ptr<Operation> op, std::vector<Ptr<BTensor>> inputs) {
  return op->get_outputs(inputs);
}

void StageProgram::find_executable_node(Ptr<BTensor> tensor) {
  for (auto op : tensor->get_child_nodes()) {
    // spdlog::info("initializing operation {} ...", op->get_name());
    if (op->check_executable()) {
      _executable_operations.push_back(op);
    }
  }
}

bool StageProgram::check_exist_in_executable(uint32_t op_id) {
  for (auto iter = _executable_operations.begin();
       iter != _executable_operations.end(); iter++) {
    if (op_id == (*iter)->get_id()) {
      return true;
    }
  }
  return false;
}

void StageProgram::finish_operation(uint32_t id) {
  _op_map[id]->set_finish();
  for (auto iter = _executable_operations.begin();
       iter != _executable_operations.end(); iter++) {
    // spdlog::info("iterating operation: {}", (*iter)->get_name());
    if (id == (*iter)->get_id()) {
      // spdlog::info("erasing operation: {}", (*iter)->get_name());
      _executable_operations.erase(iter);
      break;
    }
  }

  for (auto op : _op_map[id]->get_child_nodes()) {
    // spdlog::info("finding operation: {} / {} ", op->get_name(),
    // op->get_id());
    if (op->check_executable() && !check_exist_in_executable(op->get_id())) {
      // spdlog::info("found operation: {}", op->get_name());
      _executable_operations.push_back(op);
    }
  }
}

bool StageProgram::check_finish() {
  bool finish = true;
  for (auto const &[key, val] : _op_map) {
    finish = finish && val->check_finish();
  }

  return finish;
}

std::vector<OperationStat> StageProgram::list_operation_stat() {
  std::vector<OperationStat> ret;
  for (auto &[key, val] : _op_map) {
    ret.push_back(val->get_stat());
  }

  return ret;
}

void StageProgram::finish_operation_tile(Tile &tile) {
  _op_map[tile.operation_id]->reduce_tile(tile);
}

/**
 * logger function forStageProgram
 * TODO: log file name is tentative. think of fname rule
 */
void StageProgram::log() {
  std::string fname = Config::global_config.log_dir + "/" + _name;
  Logger::log(list_operation_stat(), fname);
}

std::vector<Ptr<BTensor>>
StageProgram::projection_block(std::vector<Ptr<BTensor>> inputs) {
  //  Multi-Head Attention (MHA) 之后的 Output Projection 层，以及随后的
  //  Residual Connection（残差连接）
  // 向模拟器的计算图中添加这两个操作（MatMul 和
  // Add），并定义它们之间的数据依赖关系。
  auto N = _breq->get_num_rows();
  auto E = Config::global_config.model_n_embd;

  std::vector<uint32_t> input_dim{N, E};
  auto res_buf = std::make_shared<NPUTensor>("residual_buffer", input_dim,
                                             NPUTensorBufType::ACT, true);

  /*
    res_buf: 创建一个名为 "residual_buffer" 的张量。
  目的：模拟残差连接（Residual Connection）中的“跳跃连接”数据。
  注意：这里直接创建了一个新的
  Tensor，而不是从之前的层传递过来。代码中有一行注释
  // fixme: residual is not with this tensor.，
  // 说明作者意识到这里只是为了模拟器构建计算图（为了让 Add
  算子有两个输入），而在功能上并没有真正连接到 Attention
  之前的输入张量。对于性能模拟（算延迟和带宽）来说，只要张量大小对，通常是可以接受的。*/

  int layer = 0; //设置当前层号为 0（这里似乎是硬编码，可能只是为了生成名字）。
  auto prefix = name_gen(LAYER(layer), BlockType::Attention);
  //生成算子名称的前缀，例如 Layer0_Attention_...。
  // auto res_buf = inputs[0];

  auto projection = add_op(
      std::make_shared<MatMul>(name_gen(prefix, OperationType::Projection),
                               _model->get_params(layer, BlockType::Attention,
                                                  OperationType::Projection)));
  inputs = get_outputs(projection, inputs);

  // fixme: residual is not with this tensor.
  auto residual =
      add_op(std::make_shared<Add>(name_gen(prefix, OperationType::Residual)));
  inputs.push_back(res_buf);
  inputs = get_outputs(residual, inputs);
  return inputs;
  /*Add: 创建一个加法算子。
  inputs.push_back(res_buf): 此时 inputs 向量里只有上一步 Projection
  的输出。这里把之前创建的 res_buf 加进去，凑够两个输入张量。 输入 1: Projection
  的输出 ($N \times E$) 输入 2: Residual Buffer ($N \times E$) get_outputs:
  模拟加法操作，inputs 更新为加法后的结果。*/
}
std::vector<Ptr<BTensor>>
StageProgram::ffn_block(std::vector<Ptr<BTensor>> inputs) {
  int layer = 0;
  auto res_buf = inputs[0];
  std::string prefix = name_gen(LAYER(layer), BlockType::FeedForward);
  // create operations
  auto ln = add_op(std::make_shared<LayerNorm>(
      name_gen(prefix, OperationType::LayerNorm),
      _model->get_params(layer, BlockType::FeedForward,
                         OperationType::LayerNorm)));
  inputs = get_outputs(ln, inputs);

  auto fc1 = add_op(std::make_shared<MatMul>(
      name_gen(prefix, OperationType::FullyConnected1),
      _model->get_params(layer, BlockType::FeedForward,
                         OperationType::FullyConnected1)));
  inputs = get_outputs(fc1, inputs);

  auto gelu =
      add_op(std::make_shared<Gelu>(name_gen(prefix, OperationType::Gelu)));
  inputs = get_outputs(gelu, inputs);

  auto fc2 = add_op(std::make_shared<MatMul>(
      name_gen(prefix, OperationType::FullyConnected2),
      _model->get_params(layer, BlockType::FeedForward,
                         OperationType::FullyConnected2)));
  inputs = get_outputs(fc2, inputs);

  auto residual =
      add_op(std::make_shared<Add>(name_gen(prefix, OperationType::Residual)));
  inputs.push_back(res_buf);
  inputs = get_outputs(residual, inputs);
  return inputs;
}

std::vector<Ptr<BTensor>>
StageProgram::qkv_gen_block(std::vector<Ptr<BTensor>> inputs) {
  int layer = 0;
  auto prefix = name_gen(LAYER(layer), BlockType::Attention);
  // 设置层号为 0。生成算子名称前缀，例如 Layer0_Attention_...。

  // (N,E) -> (N,E)
  auto ln1 = add_op(std::make_shared<LayerNorm>(
      name_gen(prefix, OperationType::LayerNorm),
      _model->get_params(layer, BlockType::Attention,
                         OperationType::LayerNorm))); // LayerNorm 的参数
  inputs = get_outputs(ln1, inputs);

  // (N,E) x (E,3E)
  auto qkv_gen = add_op(std::make_shared<MatMul>(
      name_gen(prefix, OperationType::QKVGen),
      _model->get_params(layer, BlockType::Attention, OperationType::QKVGen)));
  inputs = get_outputs(qkv_gen, inputs);

  return inputs;
}