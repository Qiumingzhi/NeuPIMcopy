阅读 NeuPIMs 项目代码（特别是 src/Model.cc 和 src/StageProgram.cc），我可以确认：NeuPIMs 的张量并行（Tensor Parallelism）切分方式与 Megatron-LM 的风格是完全一致的。

详细分析：

Megatron-LM 的核心思想是利用矩阵乘法的性质（Column Parallel 和 Row Parallel）来减少通信。 NeuPIMs 的实现细节如下：

1. Attention 层 (Self-Attention)
QKV 生成 (QKVGen):
Megatron-LM: 使用 Column Parallelism。将权重矩阵 $W_{QKV}$ 按列切分，每个 GPU 计算一部分 Head 的 Q、K、V。
NeuPIMs代码 (Model.cc):
```cpp
create_weight(..., {_config.model_n_embd, 3 * _config.model_n_embd / _config.n_tp});
```
这里输出维度（列）被 n_tp 除，意味着按列切分。这与 Megatron-LM 一致。
输出投影 (Output Projection):
Megatron-LM: 使用 Row Parallelism。输入是切分后的 Partial Results，权重矩阵 $W_O$ 按行切分。可以直接计算，最后需要一次 All-Reduce。
NeuPIMs代码 (Model.cc):
```cpp
create_weight(..., {_config.model_n_embd / _config.n_tp, _config.model_n_embd});
```
这里输入维度（行）被 n_tp 除，意味着按行切分。且在 StageProgram.cc 中可以看到 input_dim[1] /= n_tp，说明输入数据确实是切分状态。这与 Megatron-LM 一致。
2. 前馈网络层 (MLP / FFN)
第一层全连接 (FC1 / GeLU):
Megatron-LM: 使用 Column Parallelism。将 $W_1$ 按列切分。
NeuPIMs代码 (Model.cc):
```cpp
create_weight(..., {_config.model_n_embd, 4 * _config.model_n_embd / _config.n_tp});
```
输出维度扩大4倍（FFN hidden size）并被 n_tp 切分。一致。
第二层全连接 (FC2):
Megatron-LM: 使用 Row Parallelism。将 $W_2$ 按行切分。
NeuPIMs代码 (Model.cc):
```cpp
create_weight(..., {4 * _config.model_n_embd / _config.n_tp, _config.model_n_embd});
```
输入维度被 n_tp 切分。一致。
总结
NeuPIMs 完美复刻了 Megatron-LM 的 Column Parallel -> Row Parallel 的交替结构。

Attention Block: Column Parallel (QKV) -> Row Parallel (Proj)
MLP Block: Column Parallel (FC1) -> Row Parallel (FC2)
唯一的区别/注意点： 虽然切分逻辑一致，但在 StageProgram.cc 的计算图中，我并没有看到显式的 All-Reduce 算子节点。Simulator 可能是在 Row Parallel 后的 Add（残差连接）操作中隐式包含或忽略了通信开销，或者是通过其他方式（如果有互连模型）来模拟的。但就模型切分风格而言，它是标准的 Megatron-LM 风格。

